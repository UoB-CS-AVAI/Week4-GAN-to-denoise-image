{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab 4: GANs\n",
    "Welcome to COMSM0159 Advanced Visual AI (AVAI)!\n",
    "\n",
    "The goal of this labsheet is to provide an introduction to Generative Adversarial Networks (GANs) based on the week 4 lecture. By the end of this lab, you should be able to:\n",
    "1. Understand the architecture of GANs and their two components: the generator and the discriminator.\n",
    "2. Train a GAN on the MNIST dataset to generate handwritten digits.\n",
    "3. Experiment with the challenge: denoising images using MNIST dataset"
   ],
   "id": "ac693867b83a1edd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What are GANs?\n",
    "\n",
    "Generative Adversarial Networks (GANs) are a class of machine learning models where two networks, a generator and a discriminator, are trained simultaneously. The generator (\"the artist\") creates data that resembles real samples, while the discriminator (\"the art critic\") evaluates whether the data is real or fake. Two models are trained simultaneously by an adversarial process.\n",
    "\n",
    "![Generative Adversarial Networks](./intro/gan.png)\n",
    "\n",
    "During training, the generator improves its ability to create realistic images, while the discriminator becomes better at distinguishing real images from fakes. This adversarial process continues until the discriminator can no longer reliably tell the difference, meaning the generator's images are indistinguishable from real ones.\n",
    "\n",
    "![Generative Adversarial Networks](./intro/gan2.png)\n",
    "\n",
    " In today's lab, we will use the MNIST dataset to explore this process. The generator starts by producing images from random noise. As training progresses, these images gradually transform into more recognizable representations of handwritten numbers, improving their resemblance with each iteration."
   ],
   "id": "e77bece4362a6446"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load github resource in Colab\n",
    "!git clone https://github.com/UoB-CS-AVAI/Week4-GAN-to-denoise-image.git\n",
    "!mv Week4-GAN-to-denoise-image/* ./"
   ],
   "id": "94578016ec7e96fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1 - Setup: Import necessary libraries and load MNIST dataset\n",
    "You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data.\n",
    "\n",
    "- Data Loading: In PyTorch, torchvision.datasets provides the MNIST dataset. \n",
    "- DataLoader: The DataLoader in PyTorch handles batching and shuffling the data, making it easier to work with large datasets."
   ],
   "id": "d38cabb703775f52"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# For GPU (CUDA) or CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Or for ARM-based MacOS\n",
    "# device = torch.device('mps')  # Only applicable if running on a Mac with M1/M2 chip and PyTorch with MPS support\n",
    "\n",
    "# Load and preprocess the MNIST dataset; downloading if not already present.\n",
    "train_dataset = datasets.MNIST(root=\"./data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2 -  Create the models: The Generator\n",
    "In PyTorch, both the generator and discriminator models can be defined using the **nn.Sequential**.\n",
    "\n",
    "- The generator starts with **nn.Linear()** layer to project the random noise (input) into a higher-dimensional space.\n",
    "- **nn.Linear()** to further increase the dimensionality of the data, followed by reshaping it for convolutional layers.\n",
    "- **nn.ConvTranspose2d()** layers are used to upsample the data until it reaches the target image size.\n",
    "- Each layer is followed by:\n",
    "  - **nn.BatchNorm2d()** or **nn.BatchNorm2d()** to normalize the outputs of the previous layer.\n",
    "  - **nn.LeakyReLU()** activation to introduce non-linearity.\n",
    "- The output layer uses **nn.Sigmoid()** activation to constrain the output to the range [0, 1]."
   ],
   "id": "dd5690abd6103174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32 * 32)\n",
    "        self.br1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Linear(32 * 32, 128 * 7 * 7)\n",
    "        self.br2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(128 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1),  # Final upsampling to 28x28x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.br1(self.fc1(x))\n",
    "        x = self.br2(self.fc2(x))\n",
    "        # Reshape the tensor for the convolutional layers\n",
    "        x = x.reshape(-1, 128, 7, 7)\n",
    "        x = self.conv1(x)\n",
    "        output = self.conv2(x)\n",
    "        return output"
   ],
   "id": "a5b5035ce71d1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the untrained generator to create an image.",
   "id": "15db0307d5cf38fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate the generator\n",
    "# You can try to adjust this value, e.g. input_dim=50 or input_dim=128, and observe the change in the generated effect.\n",
    "generator_test = Generator(input_dim=100)\n",
    "# Set the generator to evaluation mode\n",
    "generator_test.eval()\n",
    "\n",
    "# Generate random noise as input for the generator (1 sample of 100 dimensions)\n",
    "noise = torch.randn(1, 100)\n",
    "\n",
    "# Pass the noise through the generator\n",
    "with torch.no_grad():\n",
    "    generated_images = generator_test(noise)\n",
    "\n",
    "# Convert the generated image tensor to numpy array for displaying\n",
    "print(generated_images.shape)\n",
    "generated_image = generated_images[0].detach().cpu().numpy()\n",
    "\n",
    "# Display the generated image\n",
    "plt.imshow(generated_image[0, :, :], cmap='gray')\n",
    "plt.show()"
   ],
   "id": "11af95e04eb6d8ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3 - The Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier.\n",
    "- **nn.Conv2d()** is used to apply 2D convolutions to the input image. \n",
    "- **nn.LeakyReLU()** introduces non-linearity, similar to the generator.\n",
    "- **nn.MaxPool2d()** is used to downsample the feature maps, reducing spatial dimensions and computational complexity.\n",
    "- **nn.Linear()** layers are used after flattening the feature maps to create fully connected layers.\n",
    "- The final output layer uses **nn.Sigmoid()** to constrain the output to a range between 0 and 1, representing the probability that the image is real."
   ],
   "id": "baddc096901ed98e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5, stride=1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.pl1 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 5, stride=1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.pl2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        # Output layer: input size = 1024, output size = 1 (probability of being real or fake)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pl1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pl2(x)\n",
    "        # Flatten the feature maps into a 1D vector for the fully connected layers\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ],
   "id": "cddda177c4b9f1d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the untrained discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images.",
   "id": "80b36952b4f0dd0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate the discriminator\n",
    "discriminator_test = Discriminator()\n",
    "\n",
    "# Classify the generated image\n",
    "decision = discriminator_test(generated_images)\n",
    "print(decision)"
   ],
   "id": "1ddeb86ff701f339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4 - Define the loss and optimizers\n",
    "The loss function usually uses Binary Cross Entropy (BCE) since this is a binary classification problem (real/fake images).\n",
    "\n",
    "#### Discriminator loss:\n",
    "- The discriminator loss is used to measure the ability of the discriminator to distinguish between real (label = 1) and fake (label = 0) images. \n",
    "- It compares the discriminator's predictions on real images to an array of 1s, and on fake (generated) images to an array of 0s.\n",
    "\n",
    "#### Generator loss:\n",
    "- The generator's loss quantifies how well it was able to trick the discriminator.\n",
    "- If the generator is performing well, the discriminator will classify the fake images as real (label = 1). Here, the generator's loss is calculated by comparing the discriminator's predictions on the generated images to an array of 1s.\n",
    "\n",
    "The discriminator and the generator optimizers are different since you will train two networks separately."
   ],
   "id": "54a9177b32dff075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the Binary Cross Entropy loss function\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    # Loss for real images\n",
    "    real_loss = loss_func(real_output, torch.ones_like(real_output).to(device))\n",
    "    # Loss for fake images\n",
    "    fake_loss = loss_func(fake_output, torch.zeros_like(fake_output).to(device))\n",
    "\n",
    "    loss_D = real_loss + fake_loss\n",
    "    return loss_D\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # Compare discriminator's output on fake images with target labels of 1\n",
    "    loss_G = loss_func(fake_output, torch.ones_like(fake_output).to(device))\n",
    "    return loss_G"
   ],
   "id": "f9ee6d5674a6b430",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save checkpoints: It can be helpful if a long-running training task is interrupted by saving and restoring models.",
   "id": "d3eff516fa773a14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from utils.save_checkpoint import save_checkpoint",
   "id": "ba39a4700bfd2a42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5 - Define the training loop\n",
    "- The training loop starts by generating a random noise input (seed) for the generator, which produces a fake image.\n",
    "- The discriminator is then used to classify both real images (from the training set) and fake images (generated by the generator).\n",
    "- The loss for each model (discriminator and generator) is calculated, and gradients are used to update their parameters through backpropagation."
   ],
   "id": "c82b0dc39556f73b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def training(x):\n",
    "    \n",
    "    '''Training step for the Discriminator'''\n",
    "    real_x = x.to(device)\n",
    "    real_output = gan_D(real_x)\n",
    "    fake_x = gan_G(torch.randn([batch_size, input_dim]).to(device)).detach()\n",
    "    fake_output = gan_D(fake_x)\n",
    "    loss_D =discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Backpropagate the discriminator loss and update its parameters\n",
    "    optim_D.zero_grad()\n",
    "    loss_D.backward()\n",
    "    optim_D.step()\n",
    "\n",
    "    '''Training step for the Generator'''\n",
    "    fake_x = gan_G(torch.randn([batch_size, input_dim]).to(device))\n",
    "    fake_output = gan_D(fake_x)\n",
    "    loss_G = generator_loss(fake_output)\n",
    "\n",
    "    # Backpropagate the generator loss and update its parameters\n",
    "    optim_G.zero_grad()\n",
    "    loss_G.backward()\n",
    "    optim_G.step()\n",
    "\n",
    "    return loss_D, loss_G"
   ],
   "id": "735861aef4901612",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 6 - Train the model\n",
    "- The training loop generates random noise as input for the generator to produce corresponding fake images.\n",
    "- The loss is calculated for both models, and the gradients are used to update the generator and discriminator using their respective optimizers.\n",
    "- The model is trained for a defined number of epochs, saving checkpoints and generating sample images at regular intervals.\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. "
   ],
   "id": "24367cd4b498d5d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task1: visualise the generated image at different epochs",
   "id": "f414b9a8691f5d3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# task: generate and visualise images at each epoch.\n",
    "# def visualise_generated_images(generator, input_dim, epoch, image_dir, num_images=64):\n",
    "    #     \"\"\"\n",
    "    #     Parameters: \n",
    "    #         - num_images: the number of images generated, defaults to 64.\n",
    "    #     \"\"\" \n",
    "    \n",
    "    # evaluation mode\n",
    "    \n",
    "    # random noise\n",
    "    \n",
    "    # using the generator to generate images\n",
    "\n",
    "# def display_image(epoch_no):\n",
    "    # display a single image using the epoch number"
   ],
   "id": "2423e5bcdf709235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task2: visualise the loss through a plot",
   "id": "313e4657256690d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# task: visualise the loss from the training part\n",
    "# def visualise_loss(losses_D, losses_G, image_dir, loss_type):\n",
    "    # save the loss plot"
   ],
   "id": "c32339f4fa9ba847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_dim = 100\n",
    "batch_size = 128\n",
    "num_epoch = 10\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "image_dir = './generated_image/'\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "gan_G = Generator(input_dim).to(device)\n",
    "gan_D = Discriminator().to(device)\n",
    "\n",
    "# Define separate Adam optimizers for generator and discriminator\n",
    "optim_G = torch.optim.Adam(gan_G.parameters(), lr=0.0002)\n",
    "optim_D = torch.optim.Adam(gan_D.parameters(), lr=0.0002)\n",
    "\n",
    "# Initialise the list to store the losses for each epoch\n",
    "iteration_losses_D = []\n",
    "iteration_losses_G = []\n",
    "epoch_losses_D = []\n",
    "epoch_losses_G = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    start_time = time.time()\n",
    "    total_loss_D, total_loss_G = 0, 0\n",
    "    \n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        loss_D, loss_G = training(x)\n",
    "\n",
    "        iteration_losses_D.append(loss_D.detach().item())\n",
    "        iteration_losses_G.append(loss_G.detach().item())\n",
    "        total_loss_D += loss_D.detach().item()\n",
    "        total_loss_G += loss_G.detach().item()\n",
    "        \n",
    "    epoch_losses_D.append(total_loss_D / len(train_loader))\n",
    "    epoch_losses_G.append(total_loss_G / len(train_loader))\n",
    "    \n",
    "    # Save model checkpoints\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_checkpoint(epoch + 1, gan_G, gan_D, optim_G, optim_D, checkpoint_dir)\n",
    "\n",
    "    # losses once per epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epoch}] | Loss_D {iteration_losses_D[-1]:.4f} | Loss_G {iteration_losses_G[-1]:.4f} | Time: {time.time() - start_time:.2f} sec')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epoch}]  | Loss_D {epoch_losses_D[epoch]:.4f} | Loss_G {epoch_losses_G[epoch]:.4f} | Time: {time.time() - start_time:.2f} sec')\n",
    "    \n",
    "    # Task1: visualise the generated image at different epochs\n",
    "    # visualise_generated_images(gan_G, input_dim, epoch, image_dir)\n",
    "    \n",
    "# Task2: visualise the loss through a plot\n",
    "# visualise_loss(iteration_losses_D, iteration_losses_G, image_dir, 'Iteration')\n",
    "# visualise_loss(epoch_losses_D, epoch_losses_G, image_dir, 'Epoch')\n"
   ],
   "id": "4fff123f65a980a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Challenge: Applying GANs for Denoising the MNIST images\n",
    "Using the same GAN network, adjust the code to perform image denoising. You will need to add noise to the initial dataset and retrain the GAN so that it learns the new task of generating the denoised images.\n",
    " \n",
    "You can assess the resulting denoised images through visual inspection and through the computation of MSE and PSNR metrics. MSE and PSNR are always using the original (noise free) image as a reference. Compute the MSE and PSNR of the noisy and the denoised image."
   ],
   "id": "af03f3372b72feff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Challenge - Task3: MSE and PSNR computation for the denoised image and the original image\n",
    "# Challenge - Task4: MSE and PSNR computation for the noisy image and the original image\n",
    "# def compute_mse(img1, img2):\n",
    "# \n",
    "# def compute_psnr(mse):\n",
    "    # Convert the float to a tensor"
   ],
   "id": "1f0a12205c7e449f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "from utils.GAN.generator import Generator\n",
    "from utils.GAN.discriminator import Discriminator\n",
    "from utils.GAN.loss import discriminator_loss, generator_loss\n",
    "from utils.GAN.weights_init import weights_init\n",
    "\n",
    "# use gpu or cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1 - load MNIST dataset load MNIST dataset\n",
    "\n",
    "image_size = 28 * 28  # MNIST image size\n",
    "\n",
    "# Step 2 - instantiate the generator and discriminator\n",
    "\n",
    "# Step 3 - define separate Adam optimizers\n",
    "# lr = 0.0001\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optim_G, step_size=5, gamma=0.5)\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optim_D, step_size=5, gamma=0.5)\n",
    "\n",
    "# save Directory\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "image_dir = './denoised_images'\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Initialisation of the network weights\n",
    "# generator.apply(weights_init)\n",
    "# discriminator.apply(weights_init)\n",
    "\n",
    "# Step 4 - training loop\n",
    "# def training_loop(clean_imgs):\n",
    "    # generate noise on the clean images\n",
    "    # noisy_imgs = torch.clamp(noisy_imgs, 0., 1.)\n",
    "\n",
    "    # '''Training step for the Discriminator'''\n",
    "\n",
    "    # '''Training step for the Generator'''\n",
    "\n",
    "    # return loss_D.item(), loss_G.item(), noisy_imgs, clean_imgs\n",
    "\n",
    "# Step 5 - train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss_D, total_loss_G = 0, 0\n",
    "    for batch_idx, (clean_imgs, _) in enumerate(train_loader):\n",
    "        # Get noisy_imgs and clean_imgs\n",
    "        # loss_D, loss_G, noisy_imgs, clean_imgs = training_loop(clean_imgs)\n",
    "\n",
    "        # Save only the first batch of images for visualisation\n",
    "        # if batch_idx == 0:\n",
    "    \n",
    "            # Save noisy images\n",
    "            # noisy_img_np = noisy_imgs[0].cpu().numpy()\n",
    "            # (channels, height, width) from torch, only 1 channel, because the image is greyscale\n",
    "\n",
    "            # Save denoised images\n",
    "            # denoised_img = generator(noisy_imgs.view(noisy_imgs.size(0), -1)).view(noisy_imgs.size(0), 1, 28, 28)\n",
    "            # denoised_img = torch.clamp(denoised_img, 0., 1.)\n",
    "\n",
    "            # Save clean images\n",
    "            # clean_img_np = clean_imgs[0].cpu().numpy()\n",
    "            \n",
    "            # Challenge - Compute MSE and PSNR for the noisy and denoised images\n",
    "\n",
    "            # Print MSE and PSNR values\n",
    "\n",
    "    # Learning rate updated\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "\n",
    "    # Print the average loss per epoch\n",
    "    print(f'After Epoch [{epoch + 1}/{num_epochs}], D_loss: {total_loss_D / len(train_loader):.4f}, G_loss: {total_loss_G / len(train_loader):.4f}')\n",
    "\n",
    "print(\"Training completed!\")"
   ],
   "id": "25d827cf4f89efed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a3e55766bd3969bb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
